*****************************************************Artificial Neural Network(Yapay Sinir Aglari)****************************************************

	Deep Learning'in temelini olusturur.
	ANN'in uzun yillardir populer olmamasinin sebebi yavas calismasindan kaynaklaniyor. ML Modellerine gore cok daha yavas calisiyor ve daha fazla kaynak
istiyor. GPU'larin temelini olusturan islemcilerin cok daha ucuza ve cok daha fazla yayginlasmasi ANN'in onunu acan etkenlerden biridir.


	Supervised Learning Algorithm's olan ANN, beynin ogrenme islevinin pc'ler tarafindan yapilma istegi diyebiliriz.
ANN:
	Insan beyninin ozelliklerinden olan ogrenme yolu ile yeni bilgiler turetebilme, yeni bilgiler olusturabilme ve kesfedebilme gibi yetenekleri, herhangi
bir yardim almadan otomatik olarak gerceklestirebilmek amaci ile gelistirlen pc sistemleridir.
	
	ANN, insan beyni ornek alinarak, ogrenme surecinin matematiksel olarak modellenmesi sonucu ortaya cikmistir. Beyinde ki biyolojik sinir aglarinin
yapisini ogrenme, hatirlama ve genelleme kabiliyetini taklit eder. ANN'lerde ogrenme islemi ornekler kullanilarak gerceklestirilir. Ogrenme esnasinda giris
cikis bilgileri verilerek, kurallar koyulur.


Avantajlari:

- ANN'ler bir cok hucreden meydana gelir ve bu hucreler es zamanli calisarak karmasik isleri gerceklestirir.
- Ogrenme kabiliyeti vardir ve farkli ogrenme algoritmalariyla ogrenebilirler.
- Gorulmemis ciktilar icin sonuc(bilgi) uretebilirler. Gozetimsiz ogrenim soz konusudur.
- Oruntu tanima ve siniflandirma yapabilirler. Eksik oruntuleri tamamlayabilirler.
- Hata toleransina sahiptirler. Eksik veya belirsiz bilgiyle calisabilirler. Hatali durumlarda dereceli bozulma(Graceful Degradation) gosterirler.
- Paralel calisabilmekte ve gercek zamanli bilgiyi isleyebilmektedirler.


ANN Baslica;

- Teshis,
- Siniflandirma,
- Tahmin,
- Kontrol,
- Veri Iliskilendirme,
- Veri Filtreleme,
- Yorumlama
gibi alanlarda kullanilmaktadir.


ANN Kullanim Alanlari;

- Hesaplamali Finans(Computational Finance): Kredi Skorlamasi(Credit Scoring), Algoritmik Ticaret(Algorithmic Trading)
- Goruntu Isleme ve Bilgisayarla Goru(Image Processing and Computer Vision): Yuz Tanima(Face Recognition), Hareket Tanima(Motion Detection),
									     Nesne Tanima(Object Detection)
- Hesaplamali Biyoloji(Computational Biology): Tumor Bulma(Tumor Detection), Ilac Kesfi(Durg Discovery), DNA Dizilimi(DNA Seqencing)
- Enerji Uretimi(Energy Production): Fiyat ve Yuk Tahmini(Price and Load Forecasting)
- Otomotiv, Havacilik ve Uretim(Automotive, Aerospace and Manufacturing): Ongorucu Bakim(Predictive Maintenance)
- Dogal Dil Isleme(Natural Language Processing): Sesli Asistan(Voice Assistant), Duygu Analizi(Emotion Analysis)


ANN'in Biyolojik Temelleri:

	Yapay sinir ağları nöronlardan(sinir hücrelerinden ) oluşur.
Nöronların bilgi işleme özelliği vardır. Nöronlar birbirleri ile bağlanarak fonksiyonları oluşturur.
Beynimizde 100 milyar adet nöron bulunduğu tahmin edilmekte. Bir nöron başka nöronlarla 50.000 – 250.000 arasında bağlantı yapabilmekte ve beynimizde
6×10^13’ten fazla sayıda bağlantı bulunduğu tahmin edilmektedir.

	Canlilarin davranislarini inceleyip, matematiksel olarak modelleyip benzer yapay modeller uretilmesine Sibernetik denir.Varılmak istenen nokta
eğitilebilir, kendi kendine organize olup öğrenebilen, değerlendirme yapabilen sinir ağları ile insan beyninin öğrenme ve uygulama yapısını modellemeye
çalışmaktır. Bir işi bilgisayar da gerçekleştirebilmek için algoritmasını bilmek gerekir. Algoritma, girdiyi çıktıya çevirmek için temel komut dizilerinin
tamamıdır. Ancak bazı problemleri çözmek için bilinen bir algoritma olmayabilir.İstenen ve istenmeyen durumlarda zamanla değişiklik gösterebilir ya da
kullanıcıya göre değişkenlik gösteren uygulamalar sabit algoritmalara sahip değillerdir. Elimizde bilgimiz eksikse de verimiz bol olabilir.
Kolayca hem istenen hem istenmeyen binlerce örnekten sistemin öğrenmesini sağlayabiliriz.

Günümüz teknolojisinde veri toplama cihazları sayısal olduğundan verilerin güvenilir bir şekilde erişilebiliyor olması, saklanabiliyor ve işlenebiliyor
olması bize avantaj sağlamaktadır.


Bir Sinir Hucresi;

- Dentritler,
- Hucre,
- Cekirdek,
- Akson,
- Akson Kollari ve
- Akson Terminallerinden olusur.

	Cikis terminallerinde dentrit uclarindan elde edilen sensor verilerimiz cekirdekte Agirlandirilarak akson boyunca iletiliyor ve baska sinir hucresine
baglaniyor. Bu sekilde sinirler arasi iletisim saglanmis oluyor.


Matematiksel Modeli ise;

	Dentrit dedigimiz yollar boyunca agirliklarimiz mevcut ve bu dentritlere giren bir baska norondan da gelmis olabilecek bir giris degerimiz(x0)* var.
Giris degerimiz ve dentritteki agirligimiz(w0)* carpildiktan sonra(x0w0)* sinir hucresine iletilir ve sinir hucresinde bu carpma islemi yapiliyor ve tum
dentritlerden gelen agirlik ile giris carpimlari toplanir. Yani agirlikli toplama islemi yapilir. Ardindan bir bias(b)* ile toplandiktan sonra aktivasyon
fonksiyonu ardindan cikisa aktarilir. Bu cikis nihai cikis olabilecegi gibi bir baska hucrenin girisi olabilir. Matematiksel olarak agirliklar ile girisler
carpilir arti bir bias eklenir. Boylelikle basit bir matematiksel model elde edilir.


ANN'lerde temel islem; modelin en iyi skoru verecegi w(agirlik parametresi)* ve b(bias degeri)* parametrelerinin hesabini yapmaktir.

Her bir sinir hucresi ayni sekilde hesaplanir ve bunlar birbirine seri ya da paralel baglanir.


Bir yapay sinir hucresi bes bolumden olusur. Bunlar:

1. Girdiler: Girdiler noronlara gelen verilerdir. Bu girdilerden gelen veriler biyolojik sinir hucrelerinde old. gibi toplanmak uzere noron cekirdegine
gonderilir.

2. Agirliklar: Yapay Sinir Hucresine gelen girdiler uzerinden cekirdege ulasmadan once geldikleri baglantilarin agirligiyla carpilarak cekirdege iletilir.
Bu sayede girdilerin uretilecek cikti uzerindeki etkisi ayarlanabilinmektedir.

3. Toplama Fonksiyonu(Birlestirme Fonksiyonu): Toplama fonksiyonu bir yapay sinir hucresine agirliklarla carpilarak gelen girdileri toplayarak o hucrenin
net girdisini hesaplayan bir fonksiyondur.

4. Aktivasyon Fonksiyonu: Onceki katmandaki tum girdilerin agirlikli toplamini alan ve daha sonra bir cikis degeri(tipik olarak dogrusal olmayan)
ureten ve bir sonraki katmana geciren bir fonksiyondur.(Ornegin; ReLU veya Sigmoid)

5. Ciktilar: Aktivasyon Fonksiyonundan cikan deger hucrenin cikti degeri olmaktadir. Her hucrenin birden fazla girdisi olmasina ragmen bir tek ciktisi olmalidir.
Bu cikti istenilen sayida hucreye baglanabilir.


Yapay Sinir Aglari 3 ana katmanda incelenir. Giris Katmani, Ara(Gizli) Katmanlar ve Cikis Katmani
Bilgiler ağa girdi katmanından iletilir. Ara katmanlarda işlenerek oradan çıktı katmanına gönderilirler. Bilgi işlemeden kasıt ağa gelen bilgilerin ağın
ağırlık değerleri kullanılarak çıktıya dönüştürülmesidir. Ağın girdiler için doğru çıktıları üretebilmesi için ağırlıkların doğru değerlerinin olması
gerekmektedir.


ANN; Birçok nörondan ve gizli katmandan oluşuyorsa buna çok katmanlı sinir ağı(multilayer artificial neural network) denir.
Eğer tek bir katmandan oluşuyorsa tek katmanlı sinir ağı(single layer artificial neural network) denir.


Tek Katmanlı Sinir Ağları Nasıl Çalışır?

En basit sinir ağı türü olan tek katmanlı sinir ağları mantık devrelerinde kullanılan kapılar gibi düşünülebilir. Girdisi ya doğrudan çıkışa ya da bir sonraki
katmana eklenebilir. Genellikle iki girdisi olur bir çıktısı olur. Girişler sinapslar boyunca uzanır ve ağırlıklarıyla çarpılarak bir eşik değerinden geçirilir.
İki sınıfı tek doğru ile ayıran sinir ağıdır. Ya da birçok sınıf varsa tek katmanlı birkaç sinir ağıyla gerçekleştirilen ayırtaçlar bu ayırtaçlar diskriminant
olarak tanımlanabilir. Doğrusal diskriminant denklemi ile ayırabiliyoruz.

Hesaplayacağımız  diskriminant, ağırlıklar ve girişlere bağlı bir şekilde d kadar toplama yapılarak w ağırlıkları, x ler ise girdileri ifade etmektedir.
Doğrusal diskriminant daha çok basitliği nedeniyle tercih edilen bir yöntemdir. Çıktı xj girdisinin ağırlıkları toplamıdır ve wj ağırlığının büyüklüğü xj
girdisinin önemini simgeler. Ağırlığı büyük olan girdi önemli bir girdi demektir. Girişten gelen önemli özellikleri taşıyor anlamına gelir.

Doğrusal olarak ayrılabilen iki sınıf varsa bunun için bir diskriminant denklemi kullanmak yeterli olur.


Aktivasyon Fonksiyonlari:

	ANN'e dogrusal olmayan gercek dunya ozelliklerini tanitmak icin aktivasyon fonksiyonuna ihtiyac duyariz. 
Temel olarak basit bir yapay sinir ağında x girdiler, w ağırlıklar olarak tanımlanır ve ağın çıkışına aktarılan değere f(x) yani aktivasyon işlemi uygularız.
	Eger Aktivasyon Fonksiyonu Uygulanmazsa; 
cikis sinyali basit bir dogrusal func. olur. Dogrusal Func.'lar yalnizca tek dereceli polinomlardir.
	Aktivasyon fonk. kullanilmayan bir sinir agi sinirli ogrenme gucune sahip bir dogrusal baglanim(Linear Regression) gibi davranacaktir. Ancak biz sinir
agimizin dogrusal olmayan durumlari da ogrenmesini istiyoruz. Cunku sinir agimiza ogrenmesi icin goruntu, video, yazi ve ses gibi karmasik gercek dunya
bilgileri verecegiz. 
	Cok katmanli derin sinir aglari bu sayede verilerden anlamli ozellikleri ogrenebilir.

Agirliklar ile ilgili hata degerlerini hesaplamak icin ANN'de Hatanin Geriye Yayilim Algoritmasi uygulanmaktadir. Optimizasyon stratejisini belirlemek
ve hata oranini minimize etmek gerekmektedir. Uygun Optimizasyon Algoritmasini da secmek basli basina bir konudur.

***** Her zaman yapilacak is -> GIRISLER ILE AGIRLIKLARI CARP, BIAS ILE TOPLA, AKTIVASYONU UYGULA ******


- Hemen hemen butun fonksiyonlar kullanilabilir ama bazilari daha cok kullanilir. Bunlardan bazilari:
	- Threshold Fonksiyonu(adim Fonksiyonu),
	- Sigmoid Function


Basamak Fonksiyonu(Step Function):

	İkili değer alan bir fonksiyondur ve tabiatı gereği ikili sınıflayıcı olarak kullanılır. Bu yüzden genellikle çıkış katmanlarında tercih edilir.
Gizli katmanlarda türevi öğrenme değeri temsil etmediği için kullanılması tavsiye edilmez ve zaten karşınıza da çıkmayacaktır. Peki o zaman türevlenebilir bir
fonksiyon düşünelim deyince akla hemen doğrusal (linear) fonksiyon geliyor.


Dogrusal(Linear) Fonksiyon:

	Bir dizi aktivasyon değeri üretir ve bunlar basamak fonksiyonundaki gibi ikili değerler değildir. Kesinlikle bir kaç nöronu (sinir hücresi) birbirine
bağlamaya izin verir. Fakat bu fonksiyonun önemli bir sorunu var! Türevinin sabit olması. Peki neden türevine ihtiyacımız var ve sabit olmasının olumsuzluğu
nedir? Ne demiştik; geriye yayılım (backpropagation) algoritması ile öğrenme işlemini nöronlar için gerçekleştirmiş oluyorduk. Bu algoritma türev alan bir
sistemden oluşuyor. A=c.x, x’e göre türevi alındığında c sonucuna erişiriz. Bu x ile bir ilişkinin kalmadığı anlamına gelir. Peki türevi hep sabit bir değer
çıkıyorsa öğrenme işlemi gerçekleşiyordur diyebilir miyiz? Maalesef, hayır!

	Bir başka sorun daha var! Tüm katmanlarda doğrusal fonksiyon kullanıldığında giriş katmanı ile çıkış katmanı arasında hep aynı doğrusal sonuca ulaşılır.
Doğrusal fonksiyonların doğrusal bir şekilde birleşimi yine bir başka doğrusal fonksiyondur. Bu en başta birbirine bağlayabiliriz dediğimiz nöronların yani ara
katmanların işlevsiz kaldığı anlamına gelir. Yani ilk amacımız olan çok katmanda çalışma yeteneğimizi kaybettik! 	


Sigmoid Fonksiyon:

	Dusunelim ki dogadaki cogu problem dogrusal degil ve sigmoid fonksiyonunun kombinasyonlari da dogrusal degil. Bingo!!! O halde katmanlari
siralayabiliriz. Peki ikili olmayan fonk. dusunelim. Ayni sekilde basamak fonksiyonundan farkli old. icin turevlenebilirdir. Bu da demek oluyor ki ogrenme
olayi gerceklesebilecek. x'te yapilan kucuk degisimler y'de buyuk olacaktir. Bu iyi bir siniflayici olarak kullanilabilecegi anlamina gelir. Bu fonksiyonun
bir diger avantajida dogrusal fonksiyonda old. gibi(-sonsuz, +sonsuz) ile karsilasildiginda her zaman (0,1) araliginda deger uretir. Yani Aktivasyon Degeri
ucmaz, bu guzel haberdir!!!

Sigmoid fonksiyonu en sık kullanılan aktivasyon fonksiyonu olmakla beraber birçok başka ve daha verimli alternatifleri de vardır.


Hiperbolik Tanjant(Hyperbolic Tangent-tanh): 

	Sigmoid fonksiyonuna çok benzer bir yapıya sahiptir. Ancak fonksiyonun aralığı bu kez (-1,+1) olarak tanımlanmaktadır. Sigmoid fonksiyonuna göre
avantajı ise türevinin daha dik olması yani daha çok değer alabilmesidir. Bu daha hızlı öğrenme ve sınıflama işlemi için daha geniş aralığa sahip olmasından
dolayı daha verimli olacağı anlamına gelmektedir. Ama yine fonksiyonun uçlarında gradyanların ölmesi problemi devam etmektedir.


ReLU (Rectified Linear Unit) Fonksiyonu:

	İlk bakışta pozitif eksende doğrusal fonksiyon ile aynı özelliklere sahip gibi görünecektir. Ama her şeyden önce ReLU doğada doğrusal değildir.
Aslına bakılırsa iyi bir tahmin edicidir. ReLU’nun kombinasyonları ile herhangi başka bir fonksiyona da yakınsamak mümkündür.
ReLU [0, +∞) aralığında değer alıyor, peki bunun getirileri ve götürüleri neler?
Çok fazla nöronlu büyük bir sinir ağı hayal edelim. Sigmoid ve Hiperbolik Tanjant neredeyse tüm nöronların aynı şekilde ateşlenmesine/aktive olmasına sebep
oluyordu. Bu aktivasyon yoğun yani çok işlem gerektiriyor demektir. Ağdaki bazı nöronların aktif olup, aktivasyon seyrek yani verimli bir hesaplama yükü olsun
isteriz. ReLU ile bunu sağlamış oluyoruz. Negatif eksende 0 değerlerini alması ağın daha hızlı çalışacağı anlamına da gelmektedir. Hesaplama yükünün sigmoid ve
hiperbolik tanjant fonksiyonlarına göre az olması çok katmanlı ağlarda daha çok tercih edilmesine sebep olmuştur. Süper! Fakat ReLU bile tam olarak harika değil,
neden mi? Bize işlem hızı kazandıran bu sıfır değer bölgesinin türevinin de sıfır olması! Yani öğrenmenin o bölgede gerçekleşmiyor olması.


Sızıntı (Leaky) ReLU Fonksiyonu:

	Bu sızıntı değeri 0,01 olarak verilir eğer sıfıra yakın farklı bir değer verilirse fonksiyonun adı rastgele Leaky ReLU olarak olarak değişmektedir.
(Hayır, yeni bir fonksiyon daha mı?!) Sızdırılan ReLU’nun tanım aralığı eksi sonsuza doğru devam etmektedir. Bu 0'a yakın ama 0 olmayan değer sayesinde ReLU’daki
ölen gradyanları yaşatmış yani öğrenmeyi negatif bölgedeki değerler için de sağlamış oluruz.


Softmax Fonksiyonu

	Sigmoid fonksiyonuna çok benzer bir yapıya sahiptir. Aynı Sigmoid’te olduğu gibi sınıflayıcı olarak kullanıldığında oldukça iyi bir performans sergiler.
En önemli farkı sigmoid fonksiyonu gibi ikiden fazla sınıflamak gereken durumlarda özellikle derin öğrenme modellerinin çıkış katmanında tercih edilmektedir.
Girdinin belirli sınıfa ait olma olasılığını 0–1 aralığında değerler üreterek belirlenmesini sağlamaktadır. Yani olasılıksal bir yorumlama gerçekleştirir.


Swish (A Self-Gated/Kendinden Geçitli) Fonksiyonu:

	ReLU’dan en önemli farkı negatif bölgede değer alır. E, Leaky ReLU’da aynı şekilde değer alıyordu, ondan farkı ne? Swish’in negatif bölgede aldığı
değerler doğrusal değildir! Diğer tüm aktivasyon fonksiyonları monotondur. Swish fonksiyonun çıktısının girdi arttığında bile düşebileceğine dikkat edin.
Bu ilginç ve swish’e özgü bir özelliktir.

f(x)=2x*sigmoid(beta*x)


HANGİ AKTİVASYON FONKSİYONU TERCİH EDİLMELİDİR?

	Çok sınıflayıcı fonksiyonlar olarak geniş aralıkta aktive olması dolayısıyla hiperbolik tanjant ya da modelim biraz daha yavaş öğrenebilir derseniz
sigmoid fonksiyonu kullanılabilir. Ama ağınız çok derinse ve işlem yükü önemli bir problemse ReLU tercih edilebilir.
ReLU’daki gradyanların ölmesi sorununa çözüm olarak Leaky ReLU kullanmaya karar verebilirsiniz. Ama ReLU’ya göre daha çok işlem yapmış olursunuz.

Yani aktivasyon fonksiyonu tüm bu bilgiler ve sizin yapay öğrenme modelinizin gereksinimlerine göre karar vermeniz gereken kritik bir optimizasyon problemidir.


- Ağın kolay ve hızlı yakınsaması ilk kriter olabilir.
- ReLU hız bakımından avantajlı olacaktır. Gradyanların ölmesini göze almanız gerek. Genellikle çıkış değil ara katmanlarda kullanılır.
- Gradyanların ölmesi problemine ilk çözüm Leaky ReLU olabilir.
- Derin öğrenme modelleri için ReLU ile denemelere başlamanız tavsiye edilebilir.
- Çıkış katmanlarında genellikle Softmax kullanılır.

KOD

# Gerekli kütüphanelerin import edilmesi
import math
import matplotlib.pyplot as plt
import numpy as np

# Aktivasyon fonksiyonlarının tanımlamalarının matematiksel olarak yapılması
# Sigmoid Fonksiyonu
def sigmoid(x):
    a = []
    for i in x:
        a.append(1/(1+math.exp(-i)))
    return a
# Hiperbolik Tanjant Fonksiyonu
def tanh(x, derivative=False):
    if (derivative == True):
        return (1 - (x ** 2))
    return np.tanh(x)
# ReLU Fonksiyonu
def re(x):
    b = []
    for i in x:
        if i<0:
            b.append(0)
        else:
            b.append(i)
    return b
# Leaky ReLU Fonksiyonu
def lr(x):
    b = []
    for i in x:
        if i<0:
            b.append(i/10)
        else:
            b.append(i)
    return b
  
# Grafik için oluşturulacak aralıkların belirlenmesi
x = np.arange(-3., 3., 0.1)
sig = sigmoid(x)
tanh = tanh(x)
relu = re(x)
leaky_relu = lr(x)
swish = sig*x

#Fonksiyonların ekrana çizilmesi ve gösterilmesi
line_1, = plt.plot(x,sig, label='Sigmoid')
line_2, = plt.plot(x,tanh, label='Tanh')
line_3, = plt.plot(x,relu, label='ReLU')
line_4, = plt.plot(x,leaky_relu, label='Leaky ReLU')
line_5, = plt.plot(x,swish, label='Swish')
plt.legend(handles=[line_1, line_2, line_3, line_4, line_5])
plt.axhline(y=0, color='k')
plt.axvline(x=0, color='k')
plt.show()












































