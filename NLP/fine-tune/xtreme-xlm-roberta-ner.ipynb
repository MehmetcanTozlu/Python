{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Named Entity Recognition (NER)\n\n### XLM-RoBERTa","metadata":{}},{"cell_type":"markdown","source":"## Road Map:\n* Load XTREME Dataset\n* Data Preprocessing\n* The XLM - RoBERTa Model\n* Evaluation","metadata":{}},{"cell_type":"markdown","source":"### Loading the dataset","metadata":{}},{"cell_type":"code","source":"!pip -q install datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom collections import defaultdict\nfrom datasets import DatasetDict\n\nlangs = ['de', 'fr', 'it', 'en'] # dataset'de ki bulunan diller\nfracs = [0.629, 0.229, 0.084, 0.059] # bu dillerin dataset'de ki bulunma oranlari\n\npanx_ch = defaultdict(DatasetDict)\n\nfor lang, frac in zip(langs, fracs):\n    ds = load_dataset('xtreme', name=f'PAN-X.{lang}')\n    for split in ds:\n        panx_ch[lang][split] = (ds[split].shuffle(seed=0).select(range(int(frac*ds[split].num_rows))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\npd.DataFrame({lang:[panx_ch[lang]['train'].num_rows] for lang in langs}, index=['number of training examples'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"element = panx_ch['de']['train'][0]\n\nfor key, value in element.items():\n    print(f'{key} : {value}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key, value in panx_ch['de']['train'].features.items():\n    print(f'{key} : {value}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags = panx_ch['de']['train'].features['ner_tags'].feature\nprint(tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_tag_names(batch):\n    return {'ner_tags_str': [tags.int2str(idx) for idx in batch['ner_tags']]}\n\npanx_de = panx_ch['de'].map(create_tag_names)\nde_example = panx_de['train'][0]\npd.DataFrame([de_example['tokens'], de_example['ner_tags_str']], ['Tokens', 'Tags'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\nsplit2freqs = defaultdict(Counter)\n\nfor split, dataset in panx_de.items():\n    for row in dataset['ner_tags_str']:\n        for tag in row:\n            if tag.startswith('B'):\n                tag_type = tag.split('-')[1]\n                split2freqs[split][tag_type] += 1\n\npd.DataFrame.from_dict(split2freqs, orient='index')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing\n### Multilingual Transformers kullanacagiz\n#### XLM-Roberta modelini kullanacagiz","metadata":{}},{"cell_type":"markdown","source":"#### Tokenize\n\nBert Tokenizer vs XLM-R Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nbert_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\nxlmr_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = 'Tim Sparrow lives San Diego.'\n\nbert_tokens = bert_tokenizer(text).tokens()\nxlmr_tokens = xlmr_tokenizer(text).tokens()\n\npd.DataFrame([bert_tokens, xlmr_tokens], index=['BERT', 'XLM-R'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import XLMRobertaForTokenClassification\nimport torch\n\nxlmr_model_name = 'xlm-roberta-base'\n\nindex2tag = {idx:tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag:idx for idx, tag in enumerate(tags.names)}\n\nnum_labels = tags.num_classes\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nxlmr_model = XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name,\n                                                             num_labels=num_labels,\n                                                             id2label=index2tag,\n                                                             label2id=tag2index,\n                                                             ).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = xlmr_tokenizer.encode(text, return_tensors='pt')\n\npd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=['Tokens', 'Input Ids'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = xlmr_model(input_ids.to(device)).logits\nprint(outputs.shape)\n\npredictions = torch.argmax(outputs, dim=-1)\npreds = [tags.names[p] for p in predictions[0].cpu().numpy()]\npd.DataFrame([xlmr_tokens, preds],\n            index=['Tokens', 'Tags'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Yukaridaki islemleri yapmasi icin bir fonksiyon yazalim;","metadata":{}},{"cell_type":"code","source":"def tag_text(text, tags, model, tokenizer):\n    tokens = tokenizer(text).tokens()\n    input_ids = xlmr_tokenizer.encode(text, return_tensors='pt').to(device)\n    outputs = model(input_ids)[0]\n    predictions = torch.argmax(outputs, dim=2)\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index=['Tokens', 'Tags'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words, labels = de_example['tokens'], de_example['ner_tags']\npd.DataFrame([words, labels], index=['words', 'labels'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_input = xlmr_tokenizer(de_example['tokens'], is_split_into_words=True)\n\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input['input_ids'])\n\npd.DataFrame([tokens], index=['Tokens'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_ids = tokenized_input.word_ids()\n\npd.DataFrame([tokens, word_ids], index=['Tokens', 'Word Ids'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"previous_word_idx = None\nlabel_ids = []\nfor word_idx in word_ids:\n    if word_idx is None or word_idx == previous_word_idx:\n        label_ids.append(-100)\n    elif word_idx != previous_word_idx:\n        label_ids.append(labels[word_idx])\n    previous_word_idx = word_idx\n\nlabels = [index2tag[l] if l != -100 else 'IGN' for l in label_ids]\n\nindex = ['Tokens', 'Word Ids', 'Label Ids', 'Labels']\n\npd.DataFrame([tokens, word_ids, label_ids, labels], index=index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = xlmr_tokenizer(examples['tokens'], truncation=True, is_split_into_words=True)\n    labels = []\n    for idx, label in enumerate(examples['ner_tags']):\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs['labels'] = labels\n    return tokenized_inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def encode_panx_dataset(corpus):\n    return corpus.map(tokenize_and_align_labels,\n                     batched=True,\n                     remove_columns=['langs', 'ner_tags', 'tokens'])\n\npanx_de_encoded = encode_panx_dataset(panx_ch['de'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate Metrics","metadata":{}},{"cell_type":"code","source":"!pip install -q seqeval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import classification_report\n\ny_true = [['0', '0', '0', 'B-MISC', 'I-MISC', 'I-MISC', '0', 'B-PER', 'I-PER', '0']]\ny_pred = [['0', '0', 'B-MISC', 'I-MISC', 'I-MISC', 'I-MISC', '0', 'B-PER', 'I-PER', '0']]\n\nprint(classification_report(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef align_predictions(predictions, label_ids):\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            if label_ids[batch_idx, seq_idx] != -100:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n        \n        labels_list.append(example_labels)\n        preds_list.append(example_labels)\n\n    return preds_list, labels_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 5\nbatch_size = 24\nlogging_steps = len(panx_de_encoded['train']) // batch_size\nmodel_name = 'multilingual-xlm-roberta-for-ner'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=model_name,\n    log_level=\"error\",\n    num_train_epochs=num_epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    evaluation_strategy='epoch',\n    save_steps=1e6,\n    weight_decay=0.01,\n    logging_steps=logging_steps,\n    report_to='none',\n    push_to_hub=True,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import f1_score\n\ndef compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n    \n    return {'f1': f1_score(y_true, y_pred)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorForTokenClassification","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_init():\n    return xlmr_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model_init=model_init,\n    args=training_args,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    train_dataset=panx_de_encoded['train'],\n    eval_dataset=panx_de_encoded['validation'],\n    tokenizer=xlmr_tokenizer,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()\ntrainer.push_to_hub(commit_message='Training completed.')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}