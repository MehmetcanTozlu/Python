{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d7778b-2faa-4ad4-9357-8f2520860e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de509dc-77cb-45dc-bd85-a3249b416be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f9d464b-9c59-4401-86ff-cdee50003306",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843e48c8-a0e4-4a9d-b553-21db2fb70ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MNIST.extra_repr of Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: \n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = MNIST(root='', download=False, train=True, transform=transform)\n",
    "train.extra_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff262e74-4259-4a61-b83c-758f02146024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MNIST.extra_repr of Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: \n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = MNIST(root='', download=False, train=False, transform=transform)\n",
    "test.extra_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d890e35f-5fee-453f-adeb-8bcbba713726",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_data_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8fd767-7767-4895-a24b-3fcfaea4d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # h0; LSTM'in hidden state'ini temsil eder.\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # c0; LSTM'in *long term memory* olan cell state'ini temsil eder.\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # requires_grad_(); durum tensorleri(h0, c0) ile gradient'leri takip etmeye ayarlanir.\n",
    "        # Bu, backward propagation sirasinda bu tensorler uzerinde gradyan hesaplanmasini saglar.\n",
    "        \n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        # h0.detach(), c0.detach() -> Ilk hidden state ve cell state\n",
    "        # detach(); fonksiyonu, tensorleri koparir ve gradient'leri bu tensorleri takip etmemesini saglar.\n",
    "        \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f484d658-46d7-4c0c-8456-29d89436bd92",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Pytorch'ta, herhangi bir tensorun gradyanini izlemek istiyorsak;\n",
    "#### \"requires_grad_\" ozelligini True olarak ayarlamamiz gerekir. Bu, tensor uzerinde geriye dogru gecis sirasinda gradyanin hesaplanmasini saglar.\n",
    "#### Modelin egitimi sirasinda, gradyanlar, loss func.'un turevi kullanilarak tum agirlik ve biaslara iletilir. Bu, agin ogrenmesini saglar.\n",
    "####\n",
    "#### Ancak, bazen bir durum tensorunu guncellemek istemeyebiliriz, cunku; bu tensor modelin gecmis durumunu temsil eder ve bu durumu sabit tutmak isteyebiliriz.\n",
    "#### \"detach()\" fonksiyonu, gradyanlari gecici olarak kapatmanin bir yoludur. Yani \"h0.detach()\" ile \"h0\"'in gradyani takip edilmez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1b042-4b0e-4670-90bd-17e0a1eb68c0",
   "metadata": {},
   "source": [
    "RNN ve LSTM gibi tekrarlayan nn'ler, onceki durumlarini hatirlayarak calisirlar. Bu, ozellikle zaman serileri gibi ardisik verilerle calisirken cok onemlidir.\n",
    "\n",
    "Egitim sirasinda, baslangic durumlari genellikle 0 veya kucuk Random Degerlerle baslatilirlar. Ancak, bazen baslangic durumlarini belirli bir duruma ayarlamak isteyebiliriz. Bu durumlar, modelin belirli bir baglami(ornegin, bir cumle veya bir zaman serisinin baslangici) hatirlamasina yardimci olabilir.\n",
    "\n",
    "    Bu nedenlerle, \"requires_grad_()\" ve \"detach()\" fonksiyonlari, modelin egitim surecini daha iyi kontrol etmemize ve modelin gecmis durumunu daha etkili bir sekilde yonetmemize olanak tanir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "527950f2-fcf0-4531-96f4-12d138d19fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "n_layers = 1\n",
    "output_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d737d05b-cbd8-4ad1-91fc-cbd4e1b9d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(input_dim, hidden_dim, n_layers, output_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d3200c-ca54-4035-ac3f-b3adc1d236ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "optimizer = SGD(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0c421ff9-4fe9-4cd2-b7a5-c4e06825739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30     Loss: 0.5768505334854126\n",
      "Epoch 2/30     Loss: 0.3834614157676697\n",
      "Epoch 3/30     Loss: 0.20362384617328644\n",
      "Epoch 4/30     Loss: 0.086998850107193\n",
      "Epoch 5/30     Loss: 0.2771184742450714\n",
      "Epoch 6/30     Loss: 0.029947252944111824\n",
      "Epoch 7/30     Loss: 0.08183572441339493\n",
      "Epoch 8/30     Loss: 0.016183361411094666\n",
      "Epoch 9/30     Loss: 0.07552126795053482\n",
      "Epoch 10/30     Loss: 0.01837044395506382\n",
      "Epoch 11/30     Loss: 0.022792281582951546\n",
      "Epoch 12/30     Loss: 0.04881245642900467\n",
      "Epoch 13/30     Loss: 0.06550953537225723\n",
      "Epoch 14/30     Loss: 0.013679934665560722\n",
      "Epoch 15/30     Loss: 0.02766251377761364\n",
      "Epoch 16/30     Loss: 0.11577854305505753\n",
      "Epoch 17/30     Loss: 0.02463528700172901\n",
      "Epoch 18/30     Loss: 0.01981210708618164\n",
      "Epoch 19/30     Loss: 0.01268976554274559\n",
      "Epoch 20/30     Loss: 0.05732066184282303\n",
      "Epoch 21/30     Loss: 0.028308937326073647\n",
      "Epoch 22/30     Loss: 0.020217137411236763\n",
      "Epoch 23/30     Loss: 0.010319492779672146\n",
      "Epoch 24/30     Loss: 0.0009961454197764397\n",
      "Epoch 25/30     Loss: 0.019977159798145294\n",
      "Epoch 26/30     Loss: 0.006165643222630024\n",
      "Epoch 27/30     Loss: 0.006146811880171299\n",
      "Epoch 28/30     Loss: 0.015881944447755814\n",
      "Epoch 29/30     Loss: 0.025504736229777336\n",
      "Epoch 30/30     Loss: 0.002439430681988597\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for image, label in train_data_loader:\n",
    "        image, label = image.view(-1, 28, 28).to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{n_epochs}     Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e05b527f-534a-4fb9-a7b9-25c674427e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df4c5e94-7510-420f-bb7f-b614f42dcaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for image, label in test_data_loader:\n",
    "        image, label = image.view(-1, 28, 28).to(device), label.to(device)\n",
    "        \n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "969ffafa-5ff9-4dd9-9a43-a1128aa6d556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.53000\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total) * 100\n",
    "print(f'Accuracy: {accuracy:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "653dc4f9-0069-4560-8f13-629d854b4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSingleImage(model, image):\n",
    "    \n",
    "    model.eval()\n",
    "    image = image.view(-1, 28, 28).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predict = torch.max(output.data, 1)\n",
    "    \n",
    "    return predict.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54017557-6f65-4690-8682-043a3275afff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Value: 9  Real Value: 9\n"
     ]
    }
   ],
   "source": [
    "single_image, single_label = next(iter(test_data_loader))\n",
    "\n",
    "predict_single_image = predictSingleImage(model, single_image[99])\n",
    "\n",
    "print(f'Predict Value: {predict_single_image}  Real Value: {single_label[99].item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e1a6c45-f076-47f9-b775-45c26e727e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictMultiImage(model, images):\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image in images:\n",
    "            image = image.view(-1, 28, 28).to(device)\n",
    "            output = model(image)\n",
    "            _, predict = torch.max(output.data, 1)\n",
    "            predictions.append(predict.item())\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e47e0bec-618a-442b-8b82-e8df9547f45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Value -> Predict Value: 0  Real Value: 0\n",
      "2. Value -> Predict Value: 6  Real Value: 6\n",
      "3. Value -> Predict Value: 9  Real Value: 9\n",
      "4. Value -> Predict Value: 0  Real Value: 0\n",
      "5. Value -> Predict Value: 1  Real Value: 1\n",
      "6. Value -> Predict Value: 5  Real Value: 5\n",
      "7. Value -> Predict Value: 9  Real Value: 9\n",
      "8. Value -> Predict Value: 7  Real Value: 7\n",
      "9. Value -> Predict Value: 8  Real Value: 3\n",
      "10. Value -> Predict Value: 4  Real Value: 4\n"
     ]
    }
   ],
   "source": [
    "multi_images, multi_labels = next(iter(test_data_loader))\n",
    "\n",
    "predict_multi_images = predictMultiImage(model, multi_images[10:20])\n",
    "\n",
    "for i, (predict_multi_image, true_label) in enumerate(zip(predict_multi_images, multi_labels[10:20])):\n",
    "    print(f'{i+1}. Value -> Predict Value: {predict_multi_image}  Real Value: {true_label.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72da374-6154-448a-a9a6-3d0bc0136675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
